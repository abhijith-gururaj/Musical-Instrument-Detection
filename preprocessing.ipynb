{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import os\n",
    "import sys\n",
    "import pickle \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import gc\n",
    "\n",
    "training_data = 'IRMAS-TrainingData/'\n",
    "testing_data = ['IRMAS-TestingData-Part1/Part1/', 'IRMAS-TestingData-Part2/IRTestingData-Part2/', 'IRMAS-TestingData-Part3/Part3/']\n",
    "instrument_map = {\n",
    "        \"cel\" : 0, \n",
    "        \"cla\" : 1, \n",
    "        \"flu\" : 2, \n",
    "        \"gac\" : 3, \n",
    "        \"gel\" : 4, \n",
    "        \"org\" : 5, \n",
    "        \"pia\" : 6, \n",
    "        \"sax\" : 7, \n",
    "        \"tru\" : 8, \n",
    "        \"vio\" : 9, \n",
    "        \"voi\" : 10\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(filename):\n",
    "    \"\"\"\n",
    "    Performs audio processing and generates the mel-spectrograms for the wav file.\n",
    "    \n",
    "    Returns: mel spectrogram for the wav file.\n",
    "    \"\"\" \n",
    "    fs, data = wavfile.read(filename)\n",
    "    # Convert to a mono signal by taking the mean of the left and right channels\n",
    "    audio = np.mean(data, axis=1)\n",
    "    # Downsampling from 44KHz to 22KHz\n",
    "    audio = audio[np.arange(0, audio.size, 2)]\n",
    "    # Normalize the signal\n",
    "    audio /= np.max(np.abs(audio))  \n",
    "\n",
    "    # Computing Short Time Fourier Transform\n",
    "    stft = np.abs(librosa.stft(audio, win_length=1024, hop_length=512,\n",
    "        center=True))\n",
    "    # Converting to Mel Spectogram\n",
    "    mel_spec = librosa.feature.melspectrogram(S=stft, sr=22050, n_mels=128)\n",
    "\n",
    "    # Segment the spectogram\n",
    "    seg_dur = 43\n",
    "    spec_list = []\n",
    "    for idx in range(0, mel_spec.shape[1] - seg_dur + 1, seg_dur):\n",
    "        spec_list.append(mel_spec[:, idx : (idx + seg_dur)])\n",
    "    mspecs = np.expand_dims(np.array(spec_list), axis=1)\n",
    "    \n",
    "    return mspecs\n",
    "\n",
    "def extract_features_for_audio(filename, testing, index):\n",
    "    \"\"\" \n",
    "    For a given wav file, the features are of the following format:\n",
    "    1. filename: the name of the audio file. \n",
    "    2. melspec: the mel-spectrogram of the wav file.\n",
    "    3. labels: the instruments present in the wav file. These are extracted from the filename. \n",
    "    \n",
    "    Arguments:\n",
    "        filename: the name of the wav file for processing\n",
    "        testing: whether the processing is done for testing dataset or not. Each file in testing dataset is annotated with multiple instruments\n",
    "                whereas each file in the training dataset is annotated with single instrument.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    features[\"filename\"] = filename[:-4]\n",
    "    features[\"melspec\"] = preprocess_audio(filename)\n",
    "    features[\"labels\"] = np.zeros([11])\n",
    "    \n",
    "    if testing == False:\n",
    "        features[\"labels\"][index] = 1\n",
    "    else:\n",
    "        with open(filename[:-4] + '.txt', 'r') as fp:\n",
    "            lines = fp.readlines()\n",
    "            for instrument in lines:\n",
    "                features[\"labels\"][instrument_map[instrument[:3]]] = 1\n",
    "\n",
    "    return features\n",
    "\n",
    "def preprocess_training_data():\n",
    "    \"\"\"\n",
    "    Preprocess the training data.\n",
    "    \n",
    "    Returns: the features for the training dataset.\n",
    "    \"\"\" \n",
    "    features = []\n",
    "    for instrument in instrument_map.keys() :\n",
    "\n",
    "        for root, dirs, files in os.walk(training_data + instrument):\n",
    "            total_files = len(files)\n",
    "            print(\"Processing directory: \", root, \"Total files: \", total_files)\n",
    "            count = 0\n",
    "            for file in files:\n",
    "                if file.endswith('.wav'):\n",
    "                    count += 1\n",
    "                    feat = extract_features_for_audio(training_data+instrument+ \"/\" +\n",
    "                            file,testing=False, index=instrument_map[instrument] )\n",
    "                    features.append(feat)\n",
    "                    \n",
    "    return features\n",
    "\n",
    "def preprocess_test_data():\n",
    "    \"\"\"\n",
    "    Preprocess the testing data.\n",
    "    \n",
    "    Returns: the features for the testing dataset.\n",
    "    \"\"\" \n",
    "    testing_data_features = []\n",
    "    for folder in testing_data:\n",
    "        print(\"Processing folder \", folder)\n",
    "\n",
    "        features = []\n",
    "        for root, dirs, files in os.walk(folder):\n",
    "            total_files = len(files)/2\n",
    "            print(\"Processing directory: \", root, \". Total Files: \", total_files)\n",
    "            count = 0\n",
    "            for file in files:\n",
    "                if file.endswith('.wav'):\n",
    "                    feature = extract_features_for_audio(folder+file, index=0, testing=True)\n",
    "                    features.append(feature)\n",
    "\n",
    "        testing_data_features.append(features)\n",
    "    return np.concatenate(testing_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory:  IRMAS-TrainingData/cel Total files:  388\n",
      "Processing directory:  IRMAS-TrainingData/cla Total files:  505\n",
      "Processing directory:  IRMAS-TrainingData/flu Total files:  451\n",
      "Processing directory:  IRMAS-TrainingData/gac Total files:  637\n",
      "Processing directory:  IRMAS-TrainingData/gel Total files:  760\n",
      "Processing directory:  IRMAS-TrainingData/org Total files:  682\n",
      "Processing directory:  IRMAS-TrainingData/pia Total files:  721\n",
      "Processing directory:  IRMAS-TrainingData/sax Total files:  626\n",
      "Processing directory:  IRMAS-TrainingData/tru Total files:  577\n",
      "Processing directory:  IRMAS-TrainingData/vio Total files:  580\n",
      "Processing directory:  IRMAS-TrainingData/voi Total files:  778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = preprocess_training_data()\n",
    "\n",
    "# Initialize the training feature and label matrices\n",
    "X_train = np.zeros((20115, 128, 43))\n",
    "y_train = np.zeros((20115, 11))\n",
    "\n",
    " # Fill the training data matrices \n",
    "j = 0\n",
    "for idx, _ in enumerate(training_data):\n",
    "    assert len(training_data[idx]['melspec']) == 3\n",
    "    label = training_data[idx][\"labels\"]\n",
    "    for feat in training_data[idx]['melspec']:\n",
    "        X_train[j,:,:] = feat\n",
    "        y_train[j,:] = label\n",
    "        j+=1\n",
    "        \n",
    "X_train = np.asarray(np.split(X_train, np.arange(3, X_train.shape[0]-1, 3)))\n",
    "y_train = np.asarray(np.split(y_train, np.arange(3, y_train.shape[0]-1, 3)))\n",
    "\n",
    "for y in y_train:\n",
    "    assert y.shape[0] == 3\n",
    "\n",
    "#Shuffle the dataset for randomization\n",
    "permutations = np.random.permutation(X_train.shape[0])\n",
    "X_train = np.vstack(X_train[permutations])\n",
    "y_train = np.vstack(y_train[permutations])\n",
    "\n",
    "# Split the training dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "        test_size=0.15, shuffle=False)\n",
    "# Save Numpy arrays and dictionaries\n",
    "np.save('X_train', X_train)\n",
    "np.save('y_train', y_train)\n",
    "np.save('X_val', X_val)\n",
    "np.save('y_val', y_val)\n",
    "\n",
    "# Deleting these might help in garbage collection and freeing up memory.\n",
    "del X_train\n",
    "del y_train\n",
    "del X_val\n",
    "del y_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder  IRMAS-TestingData-Part1/Part1/\n",
      "Processing directory:  IRMAS-TestingData-Part1/Part1/ . Total Files:  807.0\n",
      "Processing folder  IRMAS-TestingData-Part2/IRTestingData-Part2/\n",
      "Processing directory:  IRMAS-TestingData-Part2/IRTestingData-Part2/ . Total Files:  1301.5\n",
      "Processing folder  IRMAS-TestingData-Part3/Part3/\n",
      "Processing directory:  IRMAS-TestingData-Part3/Part3/ . Total Files:  766.0\n",
      "Data preprocessing successfully completed.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "testing_data = preprocess_test_data()\n",
    "\n",
    "# Initialize the testing feature and label dictionaries\n",
    "X_test = OrderedDict()\n",
    "y_test = OrderedDict()\n",
    "\n",
    "# Store the number of audio fragments per testing file\n",
    "# This will be used for aggrating classification on the test set\n",
    "num_fragments_per_file = [len(testing_data[i]['melspec']) \n",
    "        for i, _ in enumerate(testing_data)]\n",
    "\n",
    "# Fill the test data dictionaries\n",
    "for idx, _ in enumerate(testing_data):\n",
    "    # Initialize the feature and lable matrices for test file at index ix\n",
    "    X_test_file_ix = np.zeros((num_fragments_per_file[idx], 128, 43))\n",
    "    y_test_file_ix = np.zeros((num_fragments_per_file[idx], 11))\n",
    "\n",
    "    label = testing_data[idx][\"labels\"]\n",
    "\n",
    "    j = 0\n",
    "    for feat in testing_data[idx]['melspec']:\n",
    "        X_test_file_ix[j,:,:] = feat\n",
    "        y_test_file_ix[j,:] = label\n",
    "        j+=1\n",
    "\n",
    "    X_test[idx] = X_test_file_ix\n",
    "    y_test[idx] = y_test_file_ix\n",
    "\n",
    "# Write the testing data and labels into pickle files for further training\n",
    "f = open(\"X_test.pkl\", \"wb\")\n",
    "pickle.dump(X_test, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"y_test.pkl\", \"wb\")\n",
    "pickle.dump(y_test, f)\n",
    "f.close()  \n",
    "print(\"Data preprocessing successfully completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
